import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# ----------------------------------------------------------
# Step 1: Create Sample Dataset
# ----------------------------------------------------------
data = {
'Age': [22, 25, 29, 32, 35, 40, 45, 50],
'Salary': [25000, 30000, 40000, 42000, 50000, 55000, 60000, 62000],
'Experience_Years': [1, 2, 4, 5, 7, 9, 11, 13],
'Department': ['HR', 'Sales', 'Finance', 'Finance', 'Sales', 'HR', 'Sales', 'Finance']
}
df = pd.DataFrame(data)
print("----- ORIGINAL DATASET -----")
print(df)

# ----------------------------------------------------------
# Step 2: Creating New Features (Feature Engineering)
# ----------------------------------------------------------
# Create new column: Salary per Year of Experience (Adding 1 to Experience_Years to avoid division by zero)
df['Salary_per_Year'] = df['Salary'] / (df['Experience_Years'] + 1)

# Create Age Group Feature (Categorical binning)
df['Age_Group'] = pd.cut(df['Age'],
bins=[20, 30, 40, 50],
labels=['Young', 'Mid-age', 'Senior'])
print("\n----- AFTER CREATING NEW FEATURES -----")
print(df)

# ----------------------------------------------------------
# Step 3: Encoding Categorical Variables
# ----------------------------------------------------------
# 3a. Label Encoding for 'Department'
label_encoder = LabelEncoder()
df['Dept_Label'] = label_encoder.fit_transform(df['Department'])

# 3b. One-Hot Encoding for 'Department' and 'Age_Group'
# drop_first=True prevents multicollinearity by dropping one dummy variable for each category
df_encoded = pd.get_dummies(df, columns=['Department', 'Age_Group'], drop_first=True)
print("\n----- AFTER ENCODING CATEGORICAL VARIABLES -----")
print(df_encoded)

# ----------------------------------------------------------
# Step 4: Feature Selection
# ----------------------------------------------------------
# Define independent variables (features, excluding 'Salary') and target (y)
X = df_encoded.drop(['Salary'], axis=1)
y = df_encoded['Salary']

# Use SelectKBest with f_regression (ANOVA F-value) to select top 3 features
selector = SelectKBest(score_func=f_regression, k=3)
X_new = selector.fit_transform(X, y)

# Get column names of selected features
selected_features = X.columns[selector.get_support()]
print("\n----- SELECTED TOP FEATURES -----")
print(selected_features)

# ----------------------------------------------------------
# Step 5: Simple Model Using Selected Features (Optional)
# ----------------------------------------------------------
# Split data using only the selected features for training and testing
X_train, X_test, y_train, y_test = train_test_split(
X[selected_features], y, test_size=0.3, random_state=42)

# Initialize and train a Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Calculate and print the R-squared score (model accuracy)
score = model.score(X_test, y_test)
print(f"\nModel Accuracy using Selected Features: {score:.2f}")

# ----------------------------------------------------------
# Step 6: Save Final Dataset (Optional)
# ----------------------------------------------------------
# Save the final encoded and feature engineered DataFrame to CSV
df_encoded.to_csv("Feature_Engineered_Data.csv", index=False)
print("\nFeature engineered dataset saved as 'Feature_Engineered_Data.csv'")
